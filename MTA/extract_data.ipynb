{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import researchpy as rp\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "#sys.path.append('/Users/alina/Desktop/MIT/code/ADHD/MTA/helper')\n",
    "from helper import rr, prep, var_dict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('/Users/alina/Desktop/MIT/code/data/output/figures/mediator_regression_14months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path('/Volumes/Samsung_T5/MIT/mta').exists():\n",
    "    data_root =     '/Volumes/Samsung_T5/MIT/mta'\n",
    "    data_derived = '/Volumes/Samsung_T5/MIT/mta/output/derived_data'\n",
    "else: \n",
    "    data_root = '/Users/alina/Desktop/MIT/code/data'\n",
    "    data_derived = '/Users/alina/Desktop/MIT/code/data/output/derived_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_var = ['src_subject_id', 'interview_date', 'interview_age', 'sex', 'site', 'days_baseline']\n",
    "baseline_var_short = ['src_subject_id', 'days_baseline']\n",
    "dtypes_baseline = { 'src_subject_id' : 'str',\n",
    "                    'interview_date': 'str' , \n",
    "                    'interview_age' : 'int64' ,\n",
    "                    'sex' : 'str', \n",
    "                    'site' : 'int64' ,\n",
    "                    'days_baseline':  'int64',\n",
    "                    'version_form': 'str'}\n",
    "\n",
    "version_form = ['version_form']\n",
    "\n",
    "qsts = ['snap', 'ssrs',  'masc', 'pc']##, 'wechsler'] #masc to many missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_file = 'snap01.txt'\n",
    "ssrs_file = 'ssrs01.txt'\n",
    "#masc_file = 'masc_p01.txt'\n",
    "parent_child_file = 'pcrc01.txt'\n",
    "wechsler_file = 'wiat_iiip201.txt'\n",
    "treat_group_file = 'treatment_groups.csv'\n",
    "#outcome variablles \n",
    "snap_vars = ['snainatx', \"snahypax\", 'snahix', 'snaoddx'] #inattention_mean, hyperactie mean\n",
    "ssrs_vars = ['sspintx', 'ssptossx']# social skills mean, internalizing mean \n",
    "masc_vars = ['masc_masctotalt']\n",
    "pc_vars = ['pcrcpax', 'pcrcprx'] # power assertion, personal closeness\n",
    "#wechsler_vars = ['w1readb','w2math','w3spell' ]\n",
    "outcomes_dict  = {'snap' : snap_vars, 'ssrs' : ssrs_vars,  'pc': pc_vars} #, 'wechsler': wechsler_vars}\n",
    "\n",
    "interaction_predictors = ['days_baseline', 'site', 'trtname'] #time, site, treatment group\n",
    "\n",
    "raters = ['Teacher', 'Parent']\n",
    "\n",
    "treat_group = pd.read_csv(Path(data_derived, treat_group_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_cd_vars =[ 'cdorodd'] # CD or ODD \n",
    "anx_vars = ['pso', 'psoi', 'pag', 'pagi', 'pga', 'pgai' ,'psa', 'psai'] #poa : overanxious disorder: see if included in alaysis\n",
    "comorb_vars = np.concatenate([odd_cd_vars, anx_vars])\n",
    "\n",
    "assist_vars = ['demo61']# public assistance \n",
    "\n",
    "prev_med_vars = ['hi_24'] #prev medication \n",
    "\n",
    "accept_vars = ['d2dresp']# initial acceptance \n",
    "med_mod_list = np.concatenate([odd_cd_vars, ['anx'], assist_vars, prev_med_vars, accept_vars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# load files, drop rows if missing date, drop duplicates \n",
    "\n",
    "snap_file = 'snap01.txt'\n",
    "ssrs_file = 'ssrs01.txt'\n",
    "masc_file = 'masc_p01.txt'\n",
    "parent_child_file = 'pcrc01.txt'\n",
    "wechsler_file = 'wiat_iiip201.txt'\n",
    "treat_group_file = 'treatment_groups.csv'\n",
    "\n",
    "treat_group = pd.read_csv(Path(data_derived, treat_group_file))\n",
    "\n",
    "\n",
    "snap = prep.get_data(Path(data_root, snap_file), columns= [baseline_var, snap_vars, version_form], treat_group= treat_group, set_dtypes= True, version_form= True, split_timepoints= True)\n",
    "ssrs = prep.get_data(Path(data_root, ssrs_file), columns= [baseline_var, ssrs_vars, version_form], treat_group= treat_group, set_dtypes= True, version_form= True, split_timepoints= True)\n",
    "pc = prep.get_data(Path(data_root, parent_child_file), columns= [baseline_var, pc_vars], treat_group= treat_group, set_dtypes= True, version_form= False, split_timepoints= True)\n",
    "masc = prep.get_data(Path(data_root,masc_file), columns= [baseline_var, masc_vars], treat_group= treat_group, set_dtypes= True, version_form= False, split_timepoints= True)\n",
    "#wechsler = prep.get_data(Path(data_root, wechsler_file), columns= [baseline_var, wechsler_vars], set_dtypes= True, version_form= False, split_timepoints= True)\n",
    "data_dict = dict(zip(qsts, [snap, ssrs, masc, pc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/frc4nxb532j7gqr3_36l6kmh0000gn/T/ipykernel_30633/3120274867.py:6: DtypeWarning: Columns (9,16,63,64,120,122,125,126,127,128,129,130,138,140,142,144,147,150,152,159,170,172,173,174,175,180,183,184,185,186,187,188,190,192,197,198,960) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  diags1 = pd.read_csv(Path(data_root, diags1_file), delimiter = '\\t', skiprows=[1])\n",
      "/var/folders/hf/frc4nxb532j7gqr3_36l6kmh0000gn/T/ipykernel_30633/3120274867.py:8: DtypeWarning: Columns (18,20,23,26,29,32,35,42,50,52,54,59,69,76,85,87,368,374,380,386,392,398) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  health_qst = pd.read_csv(Path(data_root, health_qst_file), delimiter='\\t', skiprows=[1])\n"
     ]
    }
   ],
   "source": [
    "diags1_file  = 'diagpsx01.txt' #comorbid anx and cd/odd \n",
    "demog_file = 'demgr01.txt' ##public assistance \n",
    "health_qst_file  = 'health01.txt' #prev medication \n",
    "initial_sat_file = 'debrief01.txt' #initial acceptance of treatment arm \n",
    "\n",
    "diags1 = pd.read_csv(Path(data_root, diags1_file), delimiter = '\\t', skiprows=[1])\n",
    "demog = pd.read_csv(Path(data_root, demog_file), delimiter= '\\t', skiprows=[1])\n",
    "health_qst = pd.read_csv(Path(data_root, health_qst_file), delimiter='\\t', skiprows=[1])\n",
    "init_sat = pd.read_csv(Path(data_root, initial_sat_file), delimiter='\\t', skiprows=[1])\n",
    "\n",
    "med_qsts = ['diags', 'demog', 'heath_qst', 'init_sat']\n",
    "med_qsts_dict = dict(zip(med_qsts, [diags1, demog, health_qst, init_sat]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hf/frc4nxb532j7gqr3_36l6kmh0000gn/T/ipykernel_30633/1318834801.py:1: DtypeWarning: Columns (0,1,2,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  diags1 = pd.read_csv(Path(data_root, diags1_file), delimiter = '\\t')\n",
      "/var/folders/hf/frc4nxb532j7gqr3_36l6kmh0000gn/T/ipykernel_30633/1318834801.py:3: DtypeWarning: Columns (0,1,2,6,8,9,10,11,12,13,14,15,17,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,39,41,43,45,47,49,50,51,52,53,54,55,56,58,59,60,61,63,64,66,68,70,71,73,75,76,77,78,80,82,84,85,86,87,88,89,91,93,95,97,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,327,328,329,330,331,333,334,335,336,337,339,340,341,342,343,345,346,347,348,349,351,352,353,354,355,357,358,359,360,361,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  health_qst = pd.read_csv(Path(data_root, health_qst_file), delimiter='\\t')\n"
     ]
    }
   ],
   "source": [
    "diags1 = pd.read_csv(Path(data_root, diags1_file), delimiter = '\\t')\n",
    "demog = pd.read_csv(Path(data_root, demog_file), delimiter= '\\t')\n",
    "health_qst = pd.read_csv(Path(data_root, health_qst_file), delimiter='\\t')\n",
    "init_sat = pd.read_csv(Path(data_root, initial_sat_file), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comorb = diags1[np.concatenate([baseline_var_short,version_form, comorb_vars])].drop(0)\n",
    "assist = demog[np.concatenate([baseline_var_short, assist_vars])].drop(0)\n",
    "prev_med = health_qst[np.concatenate([baseline_var_short, prev_med_vars])].drop(0)\n",
    "init_acc = init_sat[np.concatenate([baseline_var_short, accept_vars])].drop(0)\n",
    "\n",
    "med_list = ['comorb', 'assist', 'prev_med', \"init_sat\"]\n",
    "med_dict = dict(zip(med_list, [comorb, assist, prev_med, init_acc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comorb['anx'] = pd.NA\n",
    "anx_vars = [ 'pso', 'psoi', 'pag', 'pagi', 'pga', 'pgai', 'psa', 'psai']\n",
    "mask = (comorb[anx_vars].fillna(0) != 0).any(axis=1)  # Extract if any anxiety disdoers, comorb anx = True \n",
    "comorb.loc[mask, 'anx'] = 1\n",
    "mask = (comorb[anx_vars] == 0).all(axis=1)  # If none of the disorder, Comorb anx = False, else NaN \n",
    "comorb.loc[mask, 'anx'] = 0\n",
    "comorb = comorb.drop(columns=anx_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = [50, 213, 578, 912] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_subjects(df, lower_bound_time=None, select='min'):\n",
    "    if lower_bound_time is not None:\n",
    "        df = df.loc[df['days_baseline'] > lower_bound_time].copy()\n",
    "    \n",
    "    # Fix for the error - Using value_counts and filtering properly\n",
    "    duplicates = df['src_subject_id'].value_counts()[df['src_subject_id'].value_counts() > 1].index\n",
    "    duplicates_df = df[df['src_subject_id'].isin(duplicates)]\n",
    "\n",
    "    if select == 'min':\n",
    "        idx_to_keep = duplicates_df.groupby('src_subject_id')['days_baseline'].idxmin()\n",
    "    elif select == 'max':\n",
    "        idx_to_keep = duplicates_df.groupby('src_subject_id')['days_baseline'].idxmax()\n",
    "    else:\n",
    "        raise ValueError('Please specify how to select unique subjects')\n",
    "    \n",
    "    return df.loc[df.index.difference(duplicates_df.index).union(idx_to_keep)]\n",
    "\n",
    "def get_unique_subjects_split(df, timepoints_unique = None, select='min', timepoints_split = None):\n",
    "    df_split = prep.split_data_from_timepoints(df, timepoints_split)\n",
    "    \n",
    "    if timepoints_unique is not None:\n",
    "        df_split_unique = [\n",
    "            find_unique_subjects(df_timepoint, time, select) for df_timepoint, time in zip(df_split.values(), timepoints_unique)\n",
    "        ]\n",
    "    else:\n",
    "        df_split_unique = [\n",
    "            find_unique_subjects(df_timepoint, None, select) for df_timepoint in df_split.values()\n",
    "        ]\n",
    "    \n",
    "    for df_ in df_split_unique:\n",
    "        if (df_['src_subject_id'].value_counts() > 1).sum() == 0:\n",
    "            print('Success')\n",
    "        else:\n",
    "            print('Found {} duplicates remaining.'.format((df_['src_subject_id'].value_counts() > 1).sum()))\n",
    "    \n",
    "    return df_split_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_baseline_dtypes_reduced(df, dtypes_baseline):\n",
    "    for col in dtypes_baseline.keys():\n",
    "        if col in df.columns:\n",
    "            dtype = dtypes_baseline[col]\n",
    "            df[col] = df[col].astype(dtype)\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_baseline\n",
    "for col in dtypes_baseline.keys():\n",
    "    if col in comorb.columns:\n",
    "        dtype = dtypes_baseline[col]\n",
    "        comorb[col] = comorb[col].astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_subject_id    object\n",
      "days_baseline      int64\n",
      "version_form      object\n",
      "cdorodd           object\n",
      "pso               object\n",
      "psoi              object\n",
      "pag               object\n",
      "pagi              object\n",
      "pga               object\n",
      "pgai              object\n",
      "psa               object\n",
      "psai              object\n",
      "anx               object\n",
      "dtype: object\n",
      "src_subject_id    object\n",
      "days_baseline      int64\n",
      "demo61            object\n",
      "dtype: object\n",
      "src_subject_id    object\n",
      "days_baseline      int64\n",
      "hi_24             object\n",
      "dtype: object\n",
      "src_subject_id    object\n",
      "days_baseline      int64\n",
      "d2dresp           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "for med in med_dict.values():\n",
    "    set_baseline_dtypes_reduced(med, dtypes_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 213, 578, 912]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "212\n",
      "576\n",
      "906\n"
     ]
    }
   ],
   "source": [
    "comorb_split = prep.split_data_from_timepoints(comorb, timepoints)\n",
    "for data in comorb_split.values():\n",
    "    print(data['days_baseline'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "comorb_split_unique = [ find_unique_subjects(comorb_timepoint, None, select='max') for comorb_timepoint, time in zip(comorb_split.values(), timepoints) ]\n",
    "for df in comorb_split_unique:\n",
    "    print((df['src_subject_id'].value_counts() > 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "212\n",
      "576\n",
      "906\n"
     ]
    }
   ],
   "source": [
    "for data in comorb_split_unique:\n",
    "    print(data['days_baseline'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[213, 578, 912, 1095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "comorb_split_unique = get_unique_subjects_split(comorb, None, select='max', timepoints_split=timepoints)\n",
    "prev_med_unique = find_unique_subjects(prev_med)\n",
    "assist_unique = find_unique_subjects(assist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_med_dict = dict(zip(med_list, [comorb_split_unique, assist_unique, prev_med, init_sat]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comorb is only one with different values per timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Index(['src_subject_id', 'days_baseline', 'version_form', 'cdorodd', 'anx'], dtype='object'), Index(['src_subject_id', 'days_baseline', 'version_form', 'cdorodd', 'anx'], dtype='object'), Index(['src_subject_id', 'days_baseline', 'version_form', 'cdorodd', 'anx'], dtype='object'), Index(['src_subject_id', 'days_baseline', 'version_form', 'cdorodd', 'anx'], dtype='object')]\n"
     ]
    }
   ],
   "source": [
    "print([df.columns for df in comorb_split_unique] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comorb_clean = [df.drop(columns= ['days_baseline', \"version_form\"]) for df in comorb_split_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_clean = [[df.drop(columns='days_baseline') for df in [ prev_med_unique, assist_unique, init_acc]]for comorb in comorb_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, listt in enumerate(dfs_clean):\n",
    "    listt.insert(0, comorb_clean[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_mediators(data_list_1_timpoint, list_mediator_df):\n",
    "    df = [pd.merge(data, dfs_clean[0], on= 'src_subject_id') for data in list_mediator_df]\n",
    "    df2 = [pd.merge(data, dfs_clean[1], on= 'src_subject_id') for data in df]\n",
    "    df3 = [pd.merge(data, dfs_clean[2], on= 'src_subject_id') for data in df2]\n",
    "    df4 = [pd.merge(data, dfs_clean[3], on= 'src_subject_id') for data in df3]\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_subject_id</th>\n",
       "      <th>cdorodd</th>\n",
       "      <th>anx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>P1001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>P1002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>P1003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6673</th>\n",
       "      <td>P1004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6677</th>\n",
       "      <td>P1005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>P1864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>P1865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9328</th>\n",
       "      <td>P1866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9330</th>\n",
       "      <td>P1867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9332</th>\n",
       "      <td>P1868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>866 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     src_subject_id cdorodd anx\n",
       "6664          P1001     0.0   0\n",
       "6666          P1002     NaN   0\n",
       "6669          P1003     1.0   1\n",
       "6673          P1004     0.0   1\n",
       "6677          P1005     1.0   0\n",
       "...             ...     ...  ..\n",
       "9324          P1864     0.0   0\n",
       "9326          P1865     0.0   0\n",
       "9328          P1866     0.0   0\n",
       "9330          P1867     0.0   0\n",
       "9332          P1868     0.0   1\n",
       "\n",
       "[866 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_clean[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qsts_dict_merged ={}\n",
    "for qst in qsts : \n",
    "    time_dict_merged = {}\n",
    "    for time_key in data_dict[qst].keys():\n",
    "\n",
    "        for listt in dfs_clean:\n",
    "            df_merged = data_dict[qst][time_key]\n",
    "            for df in listt:\n",
    "                df_merged = pd.merge(df_merged, df, on= 'src_subject_id')\n",
    "        time_dict_merged[time_key] = df_merged\n",
    "    qsts_dict_merged[qst]  =  time_dict_merged          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snap b Index(['src_subject_id', 'interview_date', 'interview_age', 'sex', 'snainatx',\n",
      "       'snahypax', 'snaoddx', 'snahix', 'days_baseline', 'site',\n",
      "       'version_form', 'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61',\n",
      "       'd2dresp'],\n",
      "      dtype='object')\n",
      "snap 14 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex', 'snainatx',\n",
      "       'snahypax', 'snaoddx', 'snahix', 'days_baseline', 'site',\n",
      "       'version_form', 'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61',\n",
      "       'd2dresp'],\n",
      "      dtype='object')\n",
      "snap 24 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex', 'snainatx',\n",
      "       'snahypax', 'snaoddx', 'snahix', 'days_baseline', 'site',\n",
      "       'version_form', 'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61',\n",
      "       'd2dresp'],\n",
      "      dtype='object')\n",
      "snap 36 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex', 'snainatx',\n",
      "       'snahypax', 'snaoddx', 'snahix', 'days_baseline', 'site',\n",
      "       'version_form', 'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61',\n",
      "       'd2dresp'],\n",
      "      dtype='object')\n",
      "ssrs b Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'version_form', 'ssptossx', 'sspintx',\n",
      "       'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "ssrs 14 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'version_form', 'ssptossx', 'sspintx',\n",
      "       'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "ssrs 24 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'version_form', 'ssptossx', 'sspintx',\n",
      "       'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "ssrs 36 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'version_form', 'ssptossx', 'sspintx',\n",
      "       'trtname', 'cdorodd', 'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "masc b Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'masc_masctotalt', 'days_baseline', 'site', 'trtname', 'cdorodd', 'anx',\n",
      "       'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "masc 14 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'masc_masctotalt', 'days_baseline', 'site', 'trtname', 'cdorodd', 'anx',\n",
      "       'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "masc 24 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'masc_masctotalt', 'days_baseline', 'site', 'trtname', 'cdorodd', 'anx',\n",
      "       'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "masc 36 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'masc_masctotalt', 'days_baseline', 'site', 'trtname', 'cdorodd', 'anx',\n",
      "       'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "pc b Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'pcrcpax', 'pcrcprx', 'trtname', 'cdorodd',\n",
      "       'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "pc 14 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'pcrcpax', 'pcrcprx', 'trtname', 'cdorodd',\n",
      "       'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "pc 24 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'pcrcpax', 'pcrcprx', 'trtname', 'cdorodd',\n",
      "       'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n",
      "pc 36 Index(['src_subject_id', 'interview_date', 'interview_age', 'sex',\n",
      "       'days_baseline', 'site', 'pcrcpax', 'pcrcprx', 'trtname', 'cdorodd',\n",
      "       'anx', 'hi_24', 'demo61', 'd2dresp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for qst in qsts:\n",
    "    for time_key in data_dict[qst].keys():\n",
    "        print(qst, time_key, qsts_dict_merged[qst][time_key].keys())\n",
    "        qsts_dict_merged[qst][time_key].to_csv(Path(data_derived, str(qst) + \"_\" + time_key + \"_mediators.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
